Summary:

In the video lecture, we explore the final stages of the design process model, with a focus on implementation, assessing impact, delivery, and evaluation of a product. The relevant chapters from Wendell's second edition are 12 (Implementation), 13 (Determining impact), and 15 (Evaluating next steps).

Key Points discussed include:

1. Implementation: After developing a product or prototype, it's vital to test it with users and observe their behavior. The definition of success should align with user behavior that you aim to change.

2. Evaluation: Keep the Minimal Viable Action (MVA) intact while making revisions. Removing key features that support the MVA may jeopardize the product's success. Look for points where users stop and address any issues that prevent them from completing the desired actions.

3. Assessing Impact: Measure success by establishing at least two metrics that indicate whether the target outcomes have been achieved. If the product itself cannot measure the outcome directly, consider constructing a data bridge using mediating variables that predict the final desired outcome.

4. Continuous Data Collection: Keep collecting data even after the product launch to inform further developments and improvements. Use this data to understand where the product is succeeding and where it may be failing.

5. Strategy for Changes: After each release, gather data, prioritize problems based on severity and ease of resolution, integrate improvements, and prepare for launch. This creates a cycle of continuous refinement.

The lecture stresses the importance of focusing on user behavior and the product's MVA while iterating product versions and measuring the product's impact through tangible outcomes. The goal is to refine and develop effective behavior change support systems.

## Transcript

From design into code and beyond. In this video, we're going to look at the last steps of the design process model. So we're going to look a bit more on identifying the impact, the delivery and the evaluation. Okay, the chapters, if you look at Wendell second edition, is 12 implementation with a product. How you could implement that. Chapter 13, which is determining the impact with A-B testing and experience. As I mentioned before, in this course, we don't talk much about evaluation. So if you want to read it, it's up to you. I only touched it in this video a little bit about evaluation. And then evaluating the next steps. So once your product has been launched, that's what we're going to talk about in this video. And that's chapter 15. So after this video, you should be able to have a bit of an idea to know how you could evaluate it and refine your product. Okay, now testing your product. It's important that you do this. Yes, when you have developed your product or your prototypes, test this with users. Now, important during these evaluations, as Wendell mentioned, is to keep an eye on what defines success. Because you might see all kinds of things going wrong or going right. But keep in mind what are the important things. So you might see that people get a little bit lost or find something unclear or cannot do things. But in the end, did that behavior, what you want to change, did that occur? And if that occurred, then all right. Then the other things are just minor issues. If it blocks that behavior to occur, then you have a major problem. In other words, keep your eye on the ball. Focus on what defines your application as success. Wendell, as I mentioned before, is very keen to focus on users' behavior. Look and observe what they do. Don't only listen to them, what they talk and say, but also see what they actually do. Because from a kind of an American perspective, remember the behaviorism approach, to see the actual behavior and less idea about the cognitions. Now, of course, seeing also other traditions, it's good to understand why people do certain things. But make sure that you look at the things they do. Let's see. Also, the minimal viable action, yes. So once you start revising your product, make sure that your minimal viable action is still there. You might throw out things or improve things or make modifications after you have done the evaluation. You might say, oh, it didn't work. Let's move that out. But if you move that out of your product, then you break up the minimal viable action, which is essential for your success. So be very keen on that to maintain that. Because if something you drop out from the minimal viable action support, then you're at a serious risk. And look also for points that people stop and move, don't move to the next step. And the one I want to talk about in this video is an interesting case we did. Back with the research we did with the University of Leiden with people with kidney transplant. Then for a year, we monitor them, how they use the system. So the system were two important things. You had almost in the beginning daily measurement they had to do. Pick some blood, have a separate device, see a kind of reading and had to enter it in a system, an online system, which give them feedback. Now you see the breakup measurement entering. And here we found after a year that there was some breakup, that some people didn't immediately enter it. But they only wrote it down on a piece of paper apparently. And at the end of the week, they entered it. All these measurements they had throughout the week, which was kind of a problem. Because the system when you enter it would give you advice where you might have to go immediately to the hospital. Yes, so here we clearly saw that there was a break in between the flow of the user action. We had this measurement and entering, which we had hoped would be nicely connected. They were postponed. Now, look for this kind of situation because you have to solve that. Because here, if they stop halfway, your action is not completely completed, what is needed. Okay. Next, impact of the assessment. So once you want to launch your product, you want to measure your success, whether you meet your target outcome. So when we define a problem, I was really keen on, and Wendell, following Wendell's idea, it's kind of observable. The outcome, what we'll see will happen to the person or the environment. Suggestion of Wendell is to define two, at least, matrices, which you can establish whether the target outcome has been measured. And ideally, these measurements are part of your product, but cannot always be the case. So if your product is measuring, if you say, right, behavior change is about more exercising, the application is keeping track about how many steps you do, then it's easy. And you can see from the application whether you meet this kind of more exercises. But in some cases, it's not possible. And then you, yeah, you're a bit stuck, of course. So imagine we built a system for people with fear of height in virtual reality. And the actual outcome is that after you've done the therapy, a person would be able to go on buildings and staircases which are a bit high and would not avoid them. But that's not part of the application. We did have a tracking thing. So two ways you can address it. So you can look for data, maybe, that you can get it. So maybe afterwards, you might ask people that use your product, you did our therapy with VR, did you, three months later, were capable of going up the buildings or not? Now, that's one way. Another way, if you think, is to make a data bridge, in a sense. And a data bridge means that you're going to look for other data that might suggest that you reached your target outcome. So let me suggest a data bridge. So with the VR, what you try to do is that people feel that they are on a building and that they get this fear that they normally have. Because it's important to get this fear to, in virtual reality, to practice that your fear will go down over time and that you have nothing to fear, in a sense. If that happens, then the fear goes down, people learn this, and then they might feel the self-efficacy, they can do it, and then, in real life, be more comfortable to go up on these high buildings. Now, you see the bridge I'm making. Presence, I could measure. Anxiety, we could measure. And this next step, the ultimate, if they go up the building, might not be necessary to measure then. Because maybe literature already demonstrated or other experiments that people would do it. So what I then only have to measure in my product is, can it raise anxiety? Because I know anxiety is needed. There's a mediating, predicting variable for the ultimate outcome. Yeah, so you make this bridge. So in the virtual reality environment, we could measure the anxiety. Or ask them repeatedly how scared they are. So this is a bridge that you can build. So that can help. Okay, and then don't forget actually to measure it, yes? So you can build up this whole strategy to measure impact, but also then collect this data and to assess it. Now, along the lines, you might have multiple releases. And then you start for each release to think about how to improve it. So make sure you collect data on this. As I mentioned, when we did the study with Leiden University about how kidney patients actually were using a product. Then we learned over time there were some issues for some patients. And then we have ideas to solve that one, yes? So don't stop with collecting data once you have done your development process. And think, all right, we did some user studies, it's good. No, once your system is launched, keep on collecting data. Because you might have new releases and now you have extra people using it. Extra people in the field which give you maybe much more interesting data. Than in your lab where you have tested in a kind of artificial settings. Now, what I mentioned, it's important to find out where the product is failing. But also what is really working. So you don't try to do very good things if you start modifying your product. And simple things what you could do is if you know that some functionality is simply not working. Maybe the simple solution is to remove that part in your next release. But again, remember, removing it, do you still have your minimal viral action? Yes, if the functionality is crucial, you cannot just throw it away. And then make sure you do small tests and quick collect this data. Maybe you have a live data connection or you go out in the field and collect from those users your data. And this quick evaluation test you can still do. So if you make a new version of the system, test it quickly and then improve it. But before you launch it, test it before you launch it. Okay, now, when you change it, the kind of strategy what you do, gather your data. Lessons learned from the release, the current release. Then see all the problems, prioritize which are the major things and what are the minor things. Then also think about what is easy to accomplish and what has the biggest impact. So that helps you to prioritize. And then integrate the new improvement in the application. And then you launch. So you can see this kind of cycle going. Gathering information, setting priorities and then integrate it into the unique release. All right. This video brings us to the end of the design model or from the other version, the understanding discovery design and refinement. I spent a little bit less time on the last steps, phases of the model and more on the beginning. But I hope you have now a bit of an idea, of a good idea in a sense, to start developing these behavior change support systems. That's basically it for this video. I hope you enjoyed it.