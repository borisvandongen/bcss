### Summary: Algorithms Considering Future States

**Definitions and Context:**

- **Future States:** Conditions or situations that will exist after an initial decision or action has been made.
- **Q-value:** Represents the cumulative value over time, accounting for immediate outcomes and discounted future outcomes.

**Key Points from the Video:**

1. **Importance of Future States:** Considering future states is crucial when making decisions aimed at influencing behavior over time, rather than just immediate outcomes.

2. **Example Situation:**
   - Different methods to motivate physical activity (walking, running, or buying running shoes) have various immediate effectiveness levels, but their outcomes can influence future states (e.g., having running shoes affects future activity options).

3. **Calculating Q-Value:**
   - We calculate the Q-value by adding the immediate outcome measure to a discounted value of the potential future outcome. The discount factor (e.g., 0.85) reflects the importance of immediate persuasion compared to future attempts.

4. **Reinforcement Learning Algorithm:**
   - The algorithm takes into account a person's opportunity, motivation, and capability (COM-B model) to predict behavior.
   - Various actions (based on Chaldini's persuasion types) and a "do nothing" option are weighed to make decisions.
   - The goal is to optimize the effort spent on activities that aid in behavior change (e.g., quitting smoking or increasing physical activity).
   
5. **Study Results:**
   - Accounting for future states provides a clear benefit for motivating preparatory behavior changes.
   - Weighing examples on the similarity of people didn't perform as well, but the fundamental concept of future-state consideration was beneficial.

**Implications:**

- Algorithms that consider future states are more relevant for long-term behavior change interventions (e.g., apps for quitting smoking) where multiple persuasive attempts occur over time.
- They might be less or not relevant in contexts where only a single persuasive attempt is made and the future state of the person is not a concern (e.g., single transactions).

**Conclusion:**
The lecture highlighted the importance of considering future states when designing persuasive attempts in algorithms, especially in the context of behavior change over time. This is encapsulated in reinforcement learning through the computation of Q-values and is an integral part of increasing the overall efficacy of persuasive techniques. The approach was demonstrated to be effective in longitudinal studies for preparatory activities toward behavior change.

## Transcript

Hello, this video is about algorithms that consider future states, so algorithms that look into the future somehow when making decisions on how to persuade people. In our overview slide, we are now located here and we will discuss an approach that we ourselves have developed to illustrate the notion of considering future states. Why consider future states? Let's look at an example for this. Let's say we have three ways we can send messages to people. So one is that we motivate them to walk, the other one is we motivate them to run, and the third one is that we provide information on buying running shoes. And let's say that we know something else, which is that our current person, of course this is Laura even though we don't see her, doesn't have running shoes at the moment. And we also have some data, so we have observed that if we send these three different types of messages, we get these values for our outcome measure. So the outcome measure might be the amount of physical activity people do, or the extent to which people follow these suggestions. So we want a high value for our outcome measure, and we get the highest value when we motivate people to walk. Now we actually know something else as well, which is the state that people are in after we send these messages. So at the start, our person didn't have running shoes, and if we motivate them to walk, then well, they still don't have running shoes, most likely at least. But if we send them information on how to buy running shoes, or where to buy them, then probably they do have running shoes next time that we want them to go for a walk or motivate them to run. So based on this, we might want to consider, well, what is now the overall most best way most best, that's a lot, what is now overall the best way to motivate people if we look at the whole trajectory over time. So for this, we can compute something that's called the Q-value, which you might have seen in the context of reinforcement learning. So what this means is that we first take the value of our outcome measure after the first persuasive attempt, here shown in orange, and then we add to that a combination of what is called the discount factor, so this blue value, and what is shown in pink, the value of our outcome measure after the second persuasive attempt. And when we compute the sum of this, we get in purple what is called a Q-value. So this Q-value expresses somehow the overall value that we get over time. And what we do here is that we discount future value. So we multiply the future value by 0.85. And the rationale for this is that, well, if we don't persuade people right now to do something, then it might also be much harder to persuade them in the future. For example, there's this concept known as small wins, which means that helping people to make small wins right now can help them to be successful in the future. So it is a bit more important that we get people to do something right now than it is to get them to do something maybe tomorrow or even next week. And we also do this for the other type of persuasion that we could send at the current time point. And if we now compare the Q-values of all these different combinations of persuasive attempts, we see that it is overall best if we now provide information on buying running shoes. And then this person will hopefully have running shoes. And then we can actually motivate them to run. If we just motivate them to walk now, then they will never go for a run because they don't have running shoes. And while walking might be possible without running shoes, running without running shoes is probably not a good idea. So that's why that would be less effective over time. And we have now used this concept in a reinforcement learning algorithm. In this algorithm, we consider people's opportunity, motivation, and capability to do an activity. These three factors are based on the COM-B model of behavior that maybe you've seen in one of the previous lectures. And according to this COM-B model, these three factors are important predictors of behavior. So we assess these factors for people. And then we choose an action. And for that, we have three of the persuasion types by Chaldini. So authority, consensus, and commitment. So authority is something like saying that your doctor really thinks you should do this. And consensus is something like, well, everybody else is doing this. And then another action we have is action planning. So this is helping people to make a plan for doing an activity. And our fifth persuasion type is to do nothing. Because sometimes, maybe if people are already super motivated, you don't need to say anything. And if they are super against doing something, then trying to motivate them can actually make things even worse. So this is also another action that we have. And what we are trying to optimize is the effort that people spend on activities. Activities in our specific context are preparatory activities for quitting smoking and becoming more physically active. So things such as envisioning your desired future self and finding a picture that illustrates this and putting this picture in your living room so you see it every day. So just preparatory activities. And in the context of reinforcement learning, therefore, the capability, opportunity, and motivation are the state that a person is in. These different types of messages we can send are actions. And our reward is based on the effort that people spend on activities. Now our paper on the results of a longitudinal study we have done to test this approach. What we see is that considering future states definitely offers some benefit. What we also did is additionally weigh these observed transition samples. So transition samples are samples of the form state, action, reward, and next state. So if we weigh these examples based on how similar people are additionally, then that does not perform so well. But this general notion of considering future states by itself offers a benefit when trying to motivate people to do preparatory activities. So the idea of these future states algorithm is that you don't just consider the value for the outcome measure that you get after a persuasive attempt, but you also consider the situation that people will be in afterwards. And you really try to optimize over time how persuasive you are overall. And this is, of course, especially important when you have several consecutive persuasive attempts. And it's definitely less relevant, or even not applicable at all, if you only have a single persuasive attempt, or it's not applicable if you just have a single one. Because you might really not care about the state people are in afterwards as long as they buy your washing machine, or as long as they go for a run tomorrow and then your application is finished, maybe, and you, yeah, therefore don't care about people's future states. So if you have a long behavior change process, such as an app that guides people to quit smoking and you send them messages every day, then it might be a good idea to consider people's future states. Because, well, maybe you get them to do something now, but then if afterwards they're so fed up with your system, they're never going to do anything else again, then that might be good to consider. So this concludes this video on algorithms that take future states into account, and I hope to see you in the next video.